---
title: "FIS project : main notebook"
output: html_notebook
---
### 1) Cargamos los datos

Para empezar, cargamos el train set y el test set.

```{r}
df_train <-read.csv2("data/train.csv")
df_test <- read.csv2("data/test.csv")
```

Verificamos las dimensiones del dataset de entrenamiento y del dataset de test.

```{r}
dim_train = dim(df_train)
print(paste("El cunjunto de entrenamiento tiene ", dim_train[1],"valores y ", dim_train[2], "variables." ))

dim_test = dim(df_test)
print(paste("El cunjunto de test tiene ", dim_test[1],"valores y ", dim_test[2], "variables." ))

```

Visualizamos el header del dataset de entrenamiento para ver un poco los datos.

```{r}
head(df_train)
```


### 2) Exploración y visualización de los datos

Para conocer un poco más los datos vamos a visuzalizar las variables del dataset de entrenamiento.

```{r}
# Cargamos la librería tidyverse para manipular los datos más facilmente

library(tidyverse)

# Gracias a eso, ahora podemos utilizar el operador %>% para crear una pipeline en la que aplicamos varias operaciones a los datos (por ejemplo "mutate" para añadir nuevas variables, "select" para guardar solamente las variables que queremos, "filter" para filtrar los datos según valores, etc).

```

#### 2.1) Visualización de la variable "age"

Esta variable representa la edad de cada cliente. Es una variable numérica y vamos a visualizarla con un gráfico que representa en el eje "x" la edad y en el eje "y" el número de clientes que tienen esa edad. Sin embargo, como hay muchos edades diferentes vamos a crear grupos de edad para que sea más facil visualizar.

```{r}

# creamos grupos de edad porque hay muchos edades diferentes
# creamos una variable que cuenta para cada grupo de edad el número de clientes

edad <- df_train %>%
  mutate(edad_interval = cut_interval(df_train$age, length = 5)) %>%
  group_by(edad_interval) %>%
  count()

# visualizamos eso en un gráfico

 ggplot(edad, aes(x = edad_interval, y=n)) +
   geom_bar(stat="identity") +
   ggtitle("Número de clientes en función de la edad") +
   labs(x="edad", y = "número de clientes")
```
Podemos ver que la mayoria de los clientes tienen entre 25 y 60 años y muchos tienen entre 30 y 40 años. Los clientes representan bien la mayoria de la población trabajadora.

#### 2.2) Visualización de la variable "job"

Esta variable representa qué tipo de trabajo tiene cada cliente. Es una variable categórica (es decir que la variable sólo puede tomar un número finito de categorías) y nominal (es decir que no hay orden entre las diferentes categorías). Vamos a visualizarla con un gráfico circular.

```{r}

# creamos una variable que cuenta para cada job el número de clientes, y clasificamos los jobs del que tiene más clientes al que tiene menos

job <- df_train %>%
  group_by(df_train$job) %>%
  count() %>%
  arrange(desc(n))

# vamos a utilizar la paleta de colores "viridis" para la visualización (hace falta descomentar la siguiente línea si ya no está instalado)
# install.packages("viridis")
library("viridis")

# construimos el gráfico
pie(x=job$n,
    labels = job$`df_train$job`,
    radius = 1,
    main = "Número de clientes en función del job",
    col = viridis(length(job$n)))
```
Ahora podemos ver los jobs mayoritarios entre los clientes. Observamos que los clientes tienen muchos tipos de jobs, así que los datos son representativos de la población (no representan una única categoría profesional).

#### 2.3) Visualización de la variable "marital"

Esta variable representa la situación marital de cada cliente. Vamos a visuarla de la misma manera que la variable anterior porque es una variable categórica y nominal.

```{r}

# creamos un variable que cuenta para cada estado posible de la variable "marital" el número de clientes que están en este estado
# clasificamos los estados del que tiene  más clientes al que tiene menos

marital <- df_train %>%
  group_by(df_train$marital) %>%
  count() %>%
  arrange(desc(n))

# visualizamos eso en un gráfico circular (de la misma manera que para la variable "jobs")

pie(x=marital$n,
    labels = marital$`df_train$marital`,
    radius = 1,
    main = "Clientes en función del valor de la variable marital",
    col = viridis(length(marital$n)))

```

De nuevo podemos observar que los datos representan bien cada categoría de la población porque hay muchos cientes para cada categoría.

#### 2.4) Visualización de la variable "education"

Esta variable representa el nivel de educación de cada cliente. Es una variable categórica y nominal.

```{r}

nivel_educación <- df_train %>%
  group_by(df_train$education) %>%
  count() %>%
  arrange(desc(n))

pie(x=nivel_educación$n,
    labels = nivel_educación$`df_train$education`,
    radius = 1,
    main = "Nivel de educación de los clientes",
    col = viridis(length(nivel_educación$n)))


```
Otra vez podemos ver que los datos representan bien cada categoría de la población.

#### 2.5) Visualización de la variable "default"

Esta variable trata de si los clientes tienen un credito impagado (si o no). Es una variable binaria que también vamos a visualizar con un gráfico circular.

```{r}

crédito_impagado <- df_train %>%
  group_by(df_train$default) %>%
  count()

pie(x=crédito_impagado$n,
    labels = crédito_impagado$`df_train$default`,
    radius = 1,
    main = "Clientes que tienen un crédito impagado y los que no",
    col = viridis(length(crédito_impagado$n)))

```
Podemos ver que la mayoría de los clientes no tienen crédito impagado.

#### 2.6) Visualización de la variable "balance"

Esta variable representa el saldo anual medio (en euros) de los clientes.

```{r}
 
 ggplot(df_train, aes(x = balance)) +
  geom_histogram(bins = 30) +
  ggtitle("Número de clientes en función del saldo anual medio") +
  labs(x="saldo anual medio (euros)", y = "número de clientes") +
  xlim(-5000, 25000)
```
Podemos ver que la mayoría de los clientes tienen un saldo anual medio cerca de 0 (menos de 5000 euros), mientras que hay una pequeña parte de los clientes que tienen un saldo anual medio más importante. Esto parece ser representativo de la población. Además, no se ve en el gráfico porque el eje "x" se para en 25 000 pero hay algunos clientes con un saldo anual medio muy grande que podemos considerar como outliers.

#### 2.7) Visualización de la variable "housing"

Esta variable determina si el cliente tiene una propiedad (si o no).

```{r}
housing <- df_train %>%
  group_by(df_train$housing) %>%
  count()

pie(x=housing$n,
    labels = housing$`df_train$housing`,
    radius = 1,
    main = "Repartición de los clientes en función de si tienen una propiedad o no",

    col = viridis(length(housing$n)))
```
Podemos ver que tenemos muchos clientes para ambas categorías.

#### 2.8) Visualización de la variable "loan"

Esta variable determina si el cliente tiene prestamos activos (si o no).

```{r}
loan <- df_train %>%
  group_by(df_train$loan) %>%
  count()

pie(x=loan$n,
    labels = loan$`df_train$loan`,
    radius = 1,
    main = "Repartición de los clientes en función de si tienen un credito activo o no",
    col = viridis(length(loan$n)))
```
Muchos clientes no tienen credito activo pero ambas categorías están bien representadas, así que esta variable también parece ser representativo de la población.

#### 2.9) Visualización de la variable "contact"

Esta variable determina el medio por el cual el cliente fue contactado.

```{r}

contact <- df_train %>%
  group_by(df_train$contact) %>%
  count()

pie(x=contact$n,
    labels = contact$`df_train$contact`,
    radius = 1,
    main = "Número de clientes en función del medio por el cuál fueron contactados",
    col = viridis(length(contact$n)))
```
No hay mucho que interpretar de este gráfico.

#### 2.10) Visualización de la variable "day"

Esta variable representa el día del mes en el cual fue contactado el cliente. Puede ser útil tener eso porque puede tener un impacto en la decisión del cliente. Por ejemplo los clientes pueden decir mas facilmente "si" al principio del mes.

```{r}

ggplot(df_train, aes(x = day)) +
  geom_histogram(bins = 31) +
  ggtitle("Número de clientes en función del día en el cual fueron contactados") +
  labs(x="día del mes", y = "número de clientes")

```

En general, podemos observar que hay muchos clientes para cada día del mes, así que no hay desequilibrio en los datos.

#### 2.11) Visualización de la variable "duration"

Esta variable representa la duración de la llamdada (en segundos).

```{r}

ggplot(df_train, aes(x = duration)) +
  geom_histogram(bins = 20) +
  ggtitle("Número de clientes en función de la duración de la llamada") +
  labs(x="duración de la llamdada (segundos)", y = "número de clientes") +
  xlim(0,1000)

```

Sólo podemos observar que la mayoría de las llamadas duran menos de 10 minutos y muchas duran 2-3 minutos.

Las visualización de las siguientes variables es de poco interés, así que ahora solo vamos a visualizar la clase final.

#### 2.12) Visualización de la clase final

Esta variable dice si el cliente ha suscrito a un un term deposit (si o no). La visualización de esta variable permite ver si no hay desequilibrio entre las 2 clases que queremos predecir.

```{r}

clase_final <- df_train %>%
  group_by(df_train$y) %>%
  count()

pie(x=clase_final$n,
    labels = clase_final$`df_train$y`,
    radius = 1,
    main = "Número de clientes en función de si han suscrito a un un term deposit",
    col = viridis(length(clase_final$n)))

```
Podemos observar que hay un pequeño desequilibrio entre las clases pero no demasiado importante.

Para concluir sobre esta parte de visualización, podemos decir que tenemos muchos datos (más de 47 000), y además son datos muy completos que representan muchos tipos de clientes diferentes. Por lo tanto, es un buen conjunto de datos para entrenar modelos predictivos.

### 3) Aprendizaje supervisado : entrenamiento de un algoritmo de regresión

Ahora vamos a aplicar técnicas de regresión que hemos visto en clase para hacer predicciones.
Más precisamente, lo que vamos a hacer es intentar predecir el saldo anual medio de un cliente a partir de las otras variables.

#### 3.1) Preprocesamiento

Hemos visto en la parte de visualización que hay outliers en los valores del saldo anual medio. Por lo tanto, vamos a eliminar los clientes con un saldo anual medio superior a 15 000, ya que pueden impedir que el modelo se ajuste a los datos correctamente.

```{r}
datos_sin_outliers <- df_train %>%
  filter(balance < 15000)
```

#### 3.2) Regresion lineal simple

Primero vamos a ver si es posible predecir el saldo anual medio con solamente una variable que es la variable edad. Podemos suponer que es dificil hacer eso pero al menos podemos ver si hay una tendencia que hace que las personas mayores tienen un saldo anual más alto por ejemplo.

Para empezar ponemos en un gráfico el saldo anual en función de la edad.

```{r}
plot(x = datos_sin_outliers$age,
     y = datos_sin_outliers$balance,
     main = "saldo anual medio en función de la edad",
     xlab="edad",
     ylab="saldo anual medio",
     ylim=c(-5000, 15000))
```
No podemos ver muchas cosas con eso. Ahora vamos a calcular y dibujar una recta de regresión en el gráfico.

```{r}
# Calculamos la regresión
lineal_simple <- lm(balance ~ age, data = datos_sin_outliers)

# Ponemos otra vez el gráfico pero con la recta de regresión
plot(x = datos_sin_outliers$age,
     y = datos_sin_outliers$balance,
     main = "saldo anual medio en función de la edad",
     xlab="edad",
     ylab="saldo anual medio",
     ylim=c(-5000, 15000))
abline(lineal_simple$coefficients, col = "red")

```
Hay una pequeña pendiente que parece decir que las personas mayores tienen un saldo anual medio un poco más alto. Pero para evaluar la calidad de la regresión vamos a calcular una métrica de evaluación : el R_square que es un valor entre 0 (malo) y 1 (bueno). 

```{r}

# Para cada cliente intentamos predecir el saldo anual medio a partir de la edad
saldo_anual_prediccion <- predict(lineal_simple)

# Calculamos la diferencia con el valor real
diferencia <- datos_sin_outliers$balance - saldo_anual_prediccion

# Calculamos el valor del R_square
#   -> calculamos la suma de los residuos (= varianza del saldo anual medio que no se puede explicar cón la regresión)
#   -> calculamos la suma tota de la varianza del saldo anual medio
#   -> utilizamos la formula que permite calcular el R_square (= porcentaje de la varianza total que se puede explicar con la regresión)
suma_residuos <- sum(diferencia^2)
suma_total <- sum((datos_sin_outliers$balance - mean(datos_sin_outliers$balance)) ^ 2)
r_square <- 1 - (suma_residuos / suma_total)
print(paste("R_square = ", r_square))
```
Tenemos un R_square muy cerca de 0, lo que significa que nuestro modelo de regresión es muy malo y que sólo la edad no es suficiente para predecir el saldo anual medio de un cliente (lo que de hecho parece bastante lógico).

#### 3.3) Regresión multivariable

Ahora vamos a intentar predecir el saldo anual de un cliente pero mirando a todas las otras variables y no sólo la edad.

```{r}

# Construimos el modelo de regresión
multivariable <- lm(balance ~ ., data = datos_sin_outliers)
# Observamos todas las métricas
summary(multivariable)
```
El valor de R_squared es 0.06, lo que significa que solo el 6% de la variación del saldo anual medio se puede explicar con las variables de entrada, lo que significa que el modelo no se ajusta bien a los datos. Puede ser que las otras variables no tienen mucha influencia en el saldo anual medio.

```{r}
# Algunos gráficos para visualiza un poco los resultados de la regresión
par(mfrow=c(2,2))
plot(multivariable)
```
### 4) Aprendizaje supervisado : entrenamiento de clasificadores

Ahora, vamos a entrenar varios clasificadores para predecir si el cliente es interesante o no. Vamos a probar cada clasificador con el conjunto de test y despues vamos a comparar los resultados de cada uno para elegir lo mejor.

#### 4.1) Árbol de decisión

rpart (Rercursive Partitioning And Regression Trees) es una librería que permite entrenar arboles de decisión para hacer predicciones. Vamos a utilizarla para construir un árbol de decisión que permite predecir el valor de la variable "y" que representa la respuesta del cliente (si o no) para suscribir a un term deposit. 

```{r}

# descomentar las siguientes líneas si ya no están instalados los packages "rattle" y "rpart.plot"
# install.packages("rattle")
# install.packages(rpart.plot)

# construimos el árbol
library(rpart)

mytree <- rpart(
  y ~ ., 
  data = df_train, 
  method = "class"
)

# miramos al árbol construido
mytree
```


```{r}
# Trazamos el árbol de decisión
library(rpart.plot)
rpart.plot(mytree)
```

Podemos ver que las duración de la llamada y el éxito (o no) de la campaña de marketing anterior tienen mucho impacto en el resultado.

```{r}
# Hacemos predicciones sobre datos de prueba
preds <- predict(mytree, newdata = df_test, type = "class")
```

```{r}
#comprobamos la precisión del árbol de decisiones en los datos de prueba
precision = mean(preds == df_test$y)
print(paste("El árbol clasifica bien ",precision*100,"% de los datos."))
```
Este resultado parece bueno, pero no significa que el clasificador es bueno : por ejemplo si tenemos 90% de los clientes que dicen "no", un clasificador estúpido que clasifica todos los clientes como clientes que van a decir "no" tiene 90% de precisión, por eso tenemos que mirar a la matriz de confusión para verificar que el algoritmo funciona bien.

```{r}
# Matriz de confusión del árbol de decisión sobre los datos de prueba.
matriz_confusion = table(df_test$y, preds)
matriz_confusion
```

Podemos ver que el clasificador no es estúpido y clasifica bastante bien tanto los clientes que dan una respuesta positiva como los que dans una respuesta negativa. 

#### 4.2) Clasificador Knn

Ahora vamos a utilizar el algoritmo Knn (K-Nearest-Neighbors) que hemos visto en clase para hacer la clasificación.

Primero tenemos que convertir las variables categóricas en variables numéricas porque con el algoritmo KNN hace falta calcular la distancia entre los datos y para hacer eso necesitamos variables numéricas.

```{r}
# convertir variables categóricas en numéricas para los datos de entrenamiento
# install.packages("fastDummies")
library(fastDummies)
df_traind <- df_train
df_traind$default <- ifelse(df_traind$default == "yes",1,0)
df_traind$housing <- ifelse(df_traind$housing == "yes",1,0)
df_traind$loan <- ifelse(df_traind$loan == "yes",1,0)
df_traind$month <- recode(df_traind$month, "jan" = 1, "feb" = 2, "mar" = 3, "apr" = 4,  "may" = 5, "jun" = 6, "jul" = 7, "aug" = 8, "sep" = 9, "oct" = 10, "nov" = 11, "dec" = 12)
df_traind <- dummy_cols(df_traind,remove_selected_columns = T, select_columns = c("job", "marital", "education", "contact", "poutcome"))
```


```{r}
#convertir variables categóricas en numéricas para datos de prueba
df_testd <- df_test
df_testd$default <- ifelse(df_testd$default == "yes",1,0)
df_testd$housing <- ifelse(df_testd$housing == "yes",1,0)
df_testd$loan <- ifelse(df_testd$loan == "yes",1,0)

df_testd$month <- recode(df_testd$month, "jan" = 1, "feb" = 2, "mar" = 3, "apr" = 4,  "may" = 5, "jun" = 6, "jul" = 7, "aug" = 8, "sep" = 9, "oct" = 10, "nov" = 11, "dec" = 12)

df_testd <- dummy_cols(df_testd,remove_selected_columns = T, select_columns = c("job", "marital", "education", "contact", "poutcome"))
```

Ahora vamos a aplicar el algoritmo, pero para eso tenemos que elegir con qué valor de "k" (= número de vecinos más cercanos) vamos a aplicar el algoritmo.

No podemos saber de antemano cuál es el mejor valor para k, por lo tanto vamos a aplicar el algoritmo con varios valores de k y elegir el valor que da los mejores resultados. (Como entrenamos varios clasificadores es normal si la ejecución del siguiente código lleva algunos minutos)

```{r}
# # descomentar la siguiente línea si ya no está instalado el package "class"
# install.packages("class")
library(class)

# vamos a aplicar el algoritmo con valores k de 2 a 10
range <- 2:10
accs <- rep(0, length(range))

for (k in range) {
  # entrenamos el clasificador
  clasificador <- knn(train = df_traind[,-c(12)],
                      test = df_testd[, -c(12)],
                      cl = df_traind$y, k = k)
  
  # calculamos la matriz de confusión
  matriz_confusion <- table(df_testd$y, clasificador)
  
  # calculamos la precisión
  accs[k-1] <- sum(diag(matriz_confusion)) / sum(matriz_confusion)
}

# Visualizamos la precisión para cada valor de k
plot(range, accs, xlab = "k")
```
Vemos en el gráfico que tenemos buenos resultados (precisión superior al 90%) con todos los valores de k, y tenemos una precisión de más de 92% con k=2 y k=3. Vamos a calcular más precisamente la precisión que tenemos con k = 2.

```{r}
k_best = which.max(accs) + 1
precision_best = accs[which.max(accs)]
print(paste("El mejor número de vecinos es k =", k_best,"y tenemos una precisión de ",precision_best*100,"%." ))
```

Dado que el rendimiento del clasificador KNN es más alto que el del árbol de decisión, el clasificador knn se generaliza y se ajusta bien a los datos y funciona mejor que el árbol de decisión y se puede utilizar para predicciones futuras.

### 5) Aplicación de algoritmos de clustering

Para terminar, vamos a aplicar varios algoritmos de clustering para ver si el conjunto de clientes se puede dividir en varios subconjuntos con clientes que se aparecen, y comparar los resultados de estos algoritmos de clustering.
#### 5.1) Kmeans Clustering

```{r}
#aplicando kmeans clustering con k = 2
set.seed(1)
df_test2 <- df_testd
km2 <- kmeans(df_test2[,-c(12)], centers = 2, nstart = 25)
km2
```
```{r}
df_test2$cluster = factor(km2$cluster)
df_test2$cluster
centers=as.data.frame(km2$centers)
centers
```

```{r}
#Preparar grupos de balance de duración para graficar
duration_balance= ggplot(data=df_test2, aes(x=duration, y=balance, color=cluster )) + 
  geom_point() +
  geom_point(data=centers, aes(x=age,y=balance, color=as.factor(c(1,2))), 
             size=10, alpha=.3, show.legend=FALSE)#guide=FALSE)
# Trazar trama de clústeres de duration_balance
duration_balance
```



```{r}
df_test2$cluster <- ifelse(df_test2$cluster == 1,"no", "yes")
table(df_test2$y, df_test2$cluster)
```


```{r}
#aplicando kmeans clustering con k = 3
set.seed(1)
df_test3 <- df_testd
km3 <- kmeans(df_test3[,-c(12)], centers = 3, nstart = 25)
km3
```


```{r}
df_test3$cluster = factor(km3$cluster)
df_test3$cluster
centers=as.data.frame(km3$centers)
centers
```


```{r}
#Preparar grupos de balance de duración para graficar
duration_balance= ggplot(data=df_test3, aes(x=duration, y=balance, color=cluster )) + 
  geom_point() +
  geom_point(data=centers, aes(x=age,y=balance, color=as.factor(c(1,2,3))), 
             size=10, alpha=.3, show.legend=FALSE)#guide=FALSE)
#Trazar trama de clústeres de duration_balance
duration_balance
```



```{r}
library(ggpubr)
ggscatter(
  df_test3, x = "duration", y = "balance", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = "Duration",
  ylab = "Balance"
) +
  stat_mean(aes(color = cluster), size = 4)
```




```{r}
#aplicando kmeans clustering con k = 3
set.seed(1)
df_test4 <- df_testd
km4 <- kmeans(df_test4[,-c(12)], centers = 4, nstart = 25)
km4
```


```{r}
df_test4$cluster = factor(km4$cluster)
df_test4$cluster
centers=as.data.frame(km4$centers)
centers
```


```{r}
#Preparar grupos de balance de duración para graficar
duration_balance= ggplot(data=df_test4, aes(x=duration, y=balance, color=cluster )) + 
  geom_point() +
  geom_point(data=centers, aes(x=age,y=balance, color=as.factor(c(1,2,3, 4))), 
             size=10, alpha=.3, show.legend=FALSE)#guide=FALSE)
#Trazar trama de clústeres de duration_balance
duration_balance
```



```{r}
ggscatter(
  df_test4, x = "duration", y = "balance", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = "Duration",
  ylab = "Balance"
) +
  stat_mean(aes(color = cluster), size = 4)
```



A partir de las visualizaciones, observé que los datos se agrupan más claramente en dos grupos en comparación con tres o cuatro grupos.


#### 5.2) Cluseting jerárquico

```{r, warning = F}
# realizar agrupaciones jerárquicas en datos de prueba
clusters <- hclust(dist(df_testd))
group = cutree(clusters, 2)

```


```{r}
#comprobar cuántos valores cumplen con el valor real de y
group <- ifelse(group == 1, "no", "yes")
table(df_testd$y, group)
```


```{r}
# Preparar grupos de balance de duración para graficar
water_protein= ggplot(data=df_testd, aes(x=duration, y=balance, color=group)) + 
  geom_point()
  
water_protein
```



```{r}
df_testd$cluster <- group
ggscatter(
  df_testd, x = "duration", y = "balance", 
  color = "cluster", palette = "npg", ellipse = TRUE, ellipse.type = "convex",
  size = 1.5,  legend = "right", ggtheme = theme_bw(),
  xlab = "Duration",
  ylab = "Balance"
) +
  stat_mean(aes(color = cluster), size = 4)
```

De la tabla anterior, observé que el agrupamiento jerárquico agrupa casi todos los puntos de datos en 1 grupo que es no. K significa que la agrupación en clústeres también agrupa los puntos de datos como sí, por lo que K significa que la agrupación en clústeres funciona mejor que la agrupación jerárquica.



